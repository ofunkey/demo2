{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "tweets_df=load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Processing\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "#converting every row of the column into lower case \n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing Accent Characters\n",
    "def standardize_accented_chars(text):\n",
    " return unicodedata.normalize(‘NFKD’, text).encode(‘ascii’, ‘ignore’).decode(‘utf-8’, ‘ignore’)\n",
    "#testing the function on a single sample for explaination\n",
    "print(standardize_accented_chars('Sómě words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.'))\n",
    "#standardizing accented characters for every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(standardize_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs\n",
    "def get_number_of_urls(documents):\n",
    "    print(\"{:.2f}% of documents contain urls\".format(sum\n",
    "(documents.apply(lambda x:x.find('http'))>0)/len\n",
    "(documents)*100))\n",
    "# Passing the 'Tweets' column of the dataframe as the argument\n",
    "print(get_number_of_urls(tweets_df.Tweets)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    " return re.sub(r’https?:\\S*’, ‘’, text)\n",
    "#testing the function on a single sample for explaination\n",
    "print(remove_url('using https://www.google.com/ as an example'))\n",
    "#removing urls from every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding Contractions\n",
    "def expand_contractions(text):\n",
    "    expanded_words = [] \n",
    "    for word in text.split():\n",
    "       expanded_words.append(contractions.fix(word)) \n",
    "    return ‘ '.join(expanded_words)\n",
    "#testing the function on a single sample for explaination\n",
    "print(expand_contractions(\"Don't is same as do not\"))\n",
    "#expanding contractions for every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Mentions and Hashtags\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r’@\\S*’, ‘’, text)\n",
    "    return re.sub(r’#\\S*’, ‘’, text)\n",
    "#testing the function on a single sample for explaination\n",
    "print(remove_mentions_and_tags('Some random @abc and #def'))\n",
    "#removing mentions and tags from every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(remove_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only Alphabet\n",
    "def keep_only_alphabet(text):\n",
    "    return re.sub(r’[^a-z]’, ‘ ‘, text)\n",
    "#testing the function on a single sample for explaination\n",
    "print(keep_only_alphabet('Just a bit more $$processing required.Just a bit!!!'))\n",
    "#for all the rows\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(keep_only_alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords(Default+Custom) and Removing Short Words\n",
    "\n",
    "def remove_stopwords(text,nlp,custom_stop_words=None,\n",
    "remove_small_tokens=True,min_len=2):\n",
    "    # if custom stop words are provided, then add them to default stop words list\n",
    "    if custom_stop_words:\n",
    "        nlp.Defaults.stop_words |= custom_stop_words\n",
    "    \n",
    "    filtered_sentence =[] \n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.is_stop == False: \n",
    "            \n",
    "            # if small tokens have to be removed, then select only those which are longer than the min_len \n",
    "            if remove_small_tokens:\n",
    "                if len(token.text)>min_len:\n",
    "                    filtered_sentence.append(token.text)\n",
    "            else:\n",
    "                filtered_sentence.append(token.text)\n",
    "    # if after the stop word removal, words are still left in the sentence, then return the sentence as a string else return null \n",
    "    return “ “.join(filtered_sentence) if len(filtered_sentence)>0 else None\n",
    "#creating a spaCy object. \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "#removing stop-words and short words from every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(lambda x:remove_stopwords(x,nlp,{\"elon\",\"musk\",}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def lemmatize(text, nlp):\n",
    "   doc = nlp(text)\n",
    "   lemmatized_text = []\n",
    "   for token in doc:\n",
    "   lemmatized_text.append(token.lemma_)\n",
    "   return “ “.join(lemmatized_text)\n",
    "#testing the function on a single sample for explaination\n",
    "print(lemmatize('Reading NLP blog is fun.' ,nlp ))\n",
    "#Performing lemmatization on every row\n",
    "tweets_df.Tweets=tweets_df.Tweets.apply(lambda x:lemmatize(x,nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Document Matrix and Dictionary\n",
    "def generate_tokens(tweet):\n",
    "    words=[]\n",
    "    for word in tweet.split(‘ ‘):\n",
    "    # using the if condition because we introduced extra spaces during text cleaning\n",
    "    if word!=’’:\n",
    "       words.append(word)\n",
    "    return words\n",
    "#storing the generated tokens in a new column named 'words'\n",
    "tweets_df['tokens']=tweets_df.Tweets.apply(generate_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def create_dictionary(words):\n",
    "    return corpora.Dictionary(words)\n",
    "#passing the dataframe column having tokens as the argument\n",
    "id2word=create_dictionary(tweets_df.tokens)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document matrix\n",
    "def create_document_matrix(tokens,id2word):\n",
    "    corpus = []\n",
    "    for text in tokens:\n",
    "       corpus.append(id2word.doc2bow(text))\n",
    " return corpus\n",
    "#passing the dataframe column having tokens and dictionary\n",
    "corpus=create_document_matrix(tweets_df.tokens,id2word)\n",
    "print(tweets_df.tokens[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    " id2word=id2word,\n",
    " num_topics=10,\n",
    " random_state=100,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating LDA Topics\n",
    "def get_lda_topics(model, num_topics, top_n_words):\n",
    "     word_dict = {}\n",
    "     for i in range(num_topics):\n",
    "         word_dict[‘Topic # ‘ + ‘{:02d}’.format(i+1)] = [i[0] for i in model.show_topic(i, topn = top_n_words)];\n",
    " \n",
    "     return pd.DataFrame(word_dict)\n",
    "get_lda_topics(lda_model,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=”mmds”, R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/mlearning-ai/topic-modelling-with-lda-on-the-tweets-mentioning-elon-musk-687076a2c86b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-queensland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "macro-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT Better one thanjust \n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "topic_model = BERTopic(ctfidf_model=ctfidf_model)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(content)\n",
    "topics, probs = topic_model.fit_transform(content)\n",
    "topic_model.get_representative_docs(0)\n",
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(content)\n",
    "topics, probs = topic_model.fit_transform(content)\n",
    "topic_model.get_representative_docs(0)\n",
    "topic_model.visualize_barchart() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-birmingham",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(papers['preprocessed_text'])\n",
    "\n",
    "number_topics = 5\n",
    "\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi Latent Semantic Analysis\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics)\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "number_of_topics=6\n",
    "words=10\n",
    "document_list,titles=load_data(\"\",\"corpus.txt\")\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Negative Matrix Factorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=10, stop_words='english')\n",
    " \n",
    "vectorized_data = vectorizer.fit_transform(data)\n",
    "\n",
    "nmf = NMF(n_components=20, solver=\"mu\")\n",
    " \n",
    "W = nmf.fit_transform(vectorized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-scene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-forty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-identity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
